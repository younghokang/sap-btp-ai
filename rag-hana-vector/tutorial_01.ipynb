{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c76108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install \"generative-ai-hub-sdk[all]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbef00a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# please enter the Credentials from your AI core landscape.\n",
    "env_vars = {\n",
    "    'AICORE_AUTH_URL' : 'https://ai-dev-2025-jypl7lq7.authentication.us21.hana.ondemand.com',\n",
    "    'AICORE_CLIENT_ID' : 'sb-cb3e467a-adbc-4d9d-ae6b-a4c62441c6d7!b33974|xsuaa_std!b22746',\n",
    "    'AICORE_CLIENT_SECRET' : '5339fb21-1cc8-45cf-996a-ee87982f66dc$wYOmtC9o6S9ypdCwwiJR4Cr_uKysaO9c9sl_xvWLhfk=',\n",
    "    'AICORE_BASE_URL' : 'https://api.ai.prod-us21.eastus.azure.ml.hana.ondemand.com/v2',\n",
    "    'AICORE_RESOURCE_GROUP' : 'new-resource-group'\n",
    "}\n",
    "\n",
    "# Set the environment variables using `os.environ`.\n",
    "for key, value in env_vars.items():\n",
    "    os.environ[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c39672cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID', 'L1', 'L2', 'L3', 'FILENAME', 'HEADER1', 'HEADER2', 'TEXT', 'VECTOR_STR']\n"
     ]
    }
   ],
   "source": [
    "# Read data from 'GRAPH_DOCU_2503.csv' and store each row in the 'data' list\n",
    "import csv\n",
    "\n",
    "data = []\n",
    "with open('GRAPH_DOCU_2503.csv', encoding='utf-8') as csvfile:\n",
    "    csv_reader = csv.reader(csvfile)\n",
    "    for row in csv_reader:\n",
    "        try:\n",
    "            data.append(row)\n",
    "        except:\n",
    "            print(row)\n",
    "\n",
    "print(data[0])  # Print the first row of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e51d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a secure connection to an SAP HANA database using hdbcli \n",
    "# pip install hdbcli\n",
    "import hdbcli\n",
    "from hdbcli import dbapi\n",
    "\n",
    "cc = dbapi.connect(\n",
    "    address='7a1c2ce7-c54f-4137-b298-f7e93e1f50e5.hana.prod-us21.hanacloud.ondemand.com',\n",
    "    port='443',\n",
    "    user='DEVELOPER',\n",
    "    password='Bticto25!',\n",
    "    encrypt=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cf6f611a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ProgrammingError",
     "evalue": "(288, 'cannot use duplicate table name: TABLENAME_AI: line 1 col 14 (at pos 13)')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mProgrammingError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m cursor = cc.cursor()\n\u001b[32m      3\u001b[39m sql_command = \u001b[33m'''\u001b[39m\u001b[33mCREATE TABLE TABLENAME_AI(ID1 BIGINT, ID2 BIGINT, L1 NVARCHAR(3), L2 NVARCHAR(3), L3 NVARCHAR(3), FILENAME NVARCHAR(100), HEADER1 NVARCHAR(5000), HEADER2 NVARCHAR(5000), TEXT NCLOB, VECTOR_STR REAL_VECTOR);\u001b[39m\u001b[33m'''\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mcursor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql_command\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m cursor.close()\n",
      "\u001b[31mProgrammingError\u001b[39m: (288, 'cannot use duplicate table name: TABLENAME_AI: line 1 col 14 (at pos 13)')"
     ]
    }
   ],
   "source": [
    "# Create a table\n",
    "cursor = cc.cursor()\n",
    "sql_command = '''CREATE TABLE TABLENAME_AI(ID1 BIGINT, ID2 BIGINT, L1 NVARCHAR(3), L2 NVARCHAR(3), L3 NVARCHAR(3), FILENAME NVARCHAR(100), HEADER1 NVARCHAR(5000), HEADER2 NVARCHAR(5000), TEXT NCLOB, VECTOR_STR REAL_VECTOR);'''\n",
    "cursor.execute(sql_command)\n",
    "cursor.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc44df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserting data into the specified table using a prepared SQL statement with real vector conversion.\n",
    "cursor = cc.cursor()\n",
    "sql_insert = 'INSERT INTO TABLENAME_AI(ID1, ID2, L1, L2, L3, FILENAME, HEADER1, HEADER2, TEXT, VECTOR_STR) VALUES (?,?,?,?,?,?,?,?,?,TO_REAL_VECTOR(?))'\n",
    "cursor.executemany(sql_insert,data[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37fb707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.24.25040300\n",
      "base_url=None auth_url=None client_id=None client_secret=None resource_group=None ai_core_client=<ai_core_sdk.ai_core_v2_client.AICoreV2Client object at 0x169125a90>\n"
     ]
    }
   ],
   "source": [
    "# pip install hana_ml\n",
    "import hana_ml\n",
    "print(hana_ml.__version__)\n",
    "from gen_ai_hub.proxy.core.proxy_clients import get_proxy_client\n",
    "proxy_client = get_proxy_client('gen-ai-hub') # for an AI Core proxy\n",
    "\n",
    "print(proxy_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63ac71dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings\n",
    "from gen_ai_hub.proxy.native.openai import embeddings\n",
    "\n",
    "def get_embedding(input, model=\"text-embedding-ada-002\") -> str:\n",
    "    response = embeddings.create(\n",
    "      model_name=model,\n",
    "      input=input\n",
    "    )\n",
    "    return response.data[0].embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad4c4736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a vector search on the table using the specified metric and return the top k results\n",
    "cursor = cc.cursor()\n",
    "def run_vector_search(query: str, metric=\"COSINE_SIMILARITY\", k=4):\n",
    "    if metric == 'L2DISTANCE':\n",
    "        sort = 'ASC'\n",
    "    else:\n",
    "        sort = 'DESC'\n",
    "    query_vector = get_embedding(query)\n",
    "    sql = '''SELECT TOP {k} \"ID2\", \"TEXT\"\n",
    "        FROM \"TABLENAME_AI\"\n",
    "        ORDER BY \"{metric}\"(\"VECTOR_STR\", TO_REAL_VECTOR('{qv}')) {sort}'''.format(k=k, metric=metric, qv=query_vector, sort=sort)\n",
    "    cursor.execute(sql)\n",
    "    hdf = cursor.fetchall()\n",
    "    return hdf[:k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef5cca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template\n",
    "promptTemplate_fstring = \"\"\"\n",
    "You are an SAP HANA Cloud expert.\n",
    "You are provided multiple context items that are related to the prompt you have to answer.\n",
    "Use the following pieces of context to answer the question at the end. \n",
    "Context:\n",
    "{context}\n",
    "Question:\n",
    "{query}\n",
    "\"\"\"\n",
    "from langchain.prompts import PromptTemplate\n",
    "promptTemplate = PromptTemplate.from_template(promptTemplate_fstring)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e18dbc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gen_ai_hub.proxy.langchain.openai.ChatOpenAI'>\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules, and define a function to query an LLM with a formatted prompt and vector-based context\n",
    "# pip install tiktoken\n",
    "import tiktoken\n",
    "\n",
    "from gen_ai_hub.proxy.langchain.openai import ChatOpenAI\n",
    "print(ChatOpenAI)\n",
    "\n",
    "def retrieve_and_query_llm(query: str, metric='COSINE_SIMILARITY', k = 4) -> str:\n",
    "    context = ''\n",
    "    context = run_vector_search(query, metric, k)\n",
    "    prompt = promptTemplate.format(query=query, context=' '.join(str(context)))\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    num_tokens = len(encoding.encode(str(prompt)))\n",
    "    print('no of tokens'+ str(num_tokens))\n",
    "    llm = ChatOpenAI(proxy_model_name='gpt-4-32k',max_tokens = 8000)\n",
    "    response = llm.invoke(prompt).content\n",
    "    print('Query: '+ query)\n",
    "    print('\\nResponse:')\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8c3402d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No deployment found with: deployment.model_name == text-embedding-ada-002",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Query the LLM with a request about calculating the shortest path and retrieve the response\u001b[39;00m\n\u001b[32m      3\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mI want to calculate a shortest path. How do I do that?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m response = \u001b[43mretrieve_and_query_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m response\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mretrieve_and_query_llm\u001b[39m\u001b[34m(query, metric, k)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mretrieve_and_query_llm\u001b[39m(query: \u001b[38;5;28mstr\u001b[39m, metric=\u001b[33m'\u001b[39m\u001b[33mCOSINE_SIMILARITY\u001b[39m\u001b[33m'\u001b[39m, k = \u001b[32m4\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m      9\u001b[39m     context = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     context = \u001b[43mrun_vector_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     prompt = promptTemplate.format(query=query, context=\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m.join(\u001b[38;5;28mstr\u001b[39m(context)))\n\u001b[32m     12\u001b[39m     encoding = tiktoken.get_encoding(\u001b[33m\"\u001b[39m\u001b[33mcl100k_base\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mrun_vector_search\u001b[39m\u001b[34m(query, metric, k)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m      7\u001b[39m     sort = \u001b[33m'\u001b[39m\u001b[33mDESC\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m query_vector = \u001b[43mget_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m sql = \u001b[33m'''\u001b[39m\u001b[33mSELECT TOP \u001b[39m\u001b[38;5;132;01m{k}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mID2\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTEXT\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[33m    FROM \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTABLENAME_AI\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[33m    ORDER BY \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{metric}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m(\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mVECTOR_STR\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, TO_REAL_VECTOR(\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{qv}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m)) \u001b[39m\u001b[38;5;132;01m{sort}\u001b[39;00m\u001b[33m'''\u001b[39m.format(k=k, metric=metric, qv=query_vector, sort=sort)\n\u001b[32m     12\u001b[39m cursor.execute(sql)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mget_embedding\u001b[39m\u001b[34m(input, model)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_embedding\u001b[39m(\u001b[38;5;28minput\u001b[39m, model=\u001b[33m\"\u001b[39m\u001b[33mtext-embedding-ada-002\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     response = \u001b[43membeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.data[\u001b[32m0\u001b[39m].embedding\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/gen_ai_hub/proxy/native/openai/clients.py:89\u001b[39m, in \u001b[36mEmbeddings.create\u001b[39m\u001b[34m(self, input, model, deployment_id, model_name, config_id, config_name, **kwargs)\u001b[39m\n\u001b[32m     82\u001b[39m model_name = if_set(model_name, if_set(model))\n\u001b[32m     83\u001b[39m model_identification = kwargs_if_set(\n\u001b[32m     84\u001b[39m     deployment_id=deployment_id,\n\u001b[32m     85\u001b[39m     model_name=model_name,\n\u001b[32m     86\u001b[39m     config_id=config_id,\n\u001b[32m     87\u001b[39m     config_name=config_name,\n\u001b[32m     88\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m deployment = \u001b[43mproxy_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect_deployment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_identification\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m model_name = deployment.model_name \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m???\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_deployment(deployment):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/gen_ai_hub/proxy/gen_ai_hub_proxy/client.py:237\u001b[39m, in \u001b[36mGenAIHubProxyClient.select_deployment\u001b[39m\u001b[34m(self, raise_on_multiple, **search_key_value)\u001b[39m\n\u001b[32m    235\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m matched_deployments[\u001b[32m0\u001b[39m]\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mNo deployment found with: \u001b[39m\u001b[33m'\u001b[39m + \u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(\n\u001b[32m    238\u001b[39m         [\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mdeployment.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m == \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m search_key_value.items()]\n\u001b[32m    239\u001b[39m     ))\n",
      "\u001b[31mValueError\u001b[39m: No deployment found with: deployment.model_name == text-embedding-ada-002"
     ]
    }
   ],
   "source": [
    "# Query the LLM with a request about calculating the shortest path and retrieve the response\n",
    "\n",
    "query = \"I want to calculate a shortest path. How do I do that?\"\n",
    "response = retrieve_and_query_llm(query=query, k=4)\n",
    "response\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
